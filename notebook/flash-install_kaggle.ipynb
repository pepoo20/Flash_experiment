{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7468157,"sourceType":"datasetVersion","datasetId":4315875}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install -q pytorch-lightning nltk\n!pip install sentencepiece\n!pip uninstall -y ninja && pip install ninja\n!pip install wandb\n!pip install OmegaConf\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UL1qJrP-bWm4","outputId":"f2638ca0-e659-47d8-ac8a-f3a670aea53c","execution":{"iopub.status.busy":"2024-01-30T05:37:58.460177Z","iopub.execute_input":"2024-01-30T05:37:58.460480Z","iopub.status.idle":"2024-01-30T05:39:16.874495Z","shell.execute_reply.started":"2024-01-30T05:37:58.460452Z","shell.execute_reply":"2024-01-30T05:39:16.873407Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nFound existing installation: ninja 1.11.1.1\nUninstalling ninja-1.11.1.1:\n  Successfully uninstalled ninja-1.11.1.1\nCollecting ninja\n  Obtaining dependency information for ninja from https://files.pythonhosted.org/packages/6d/92/8d7aebd4430ab5ff65df2bfee6d5745f95c004284db2d8ca76dcbfd9de47/ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata\n  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\nDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: ninja\nSuccessfully installed ninja-1.11.1.1\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nCollecting OmegaConf\n  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m964.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from OmegaConf)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from OmegaConf) (6.0.1)\nBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=15d86444f5b8ef4927f9a4cb4ff80809ff1eecb9229c59d737ea4cc9fedf357c\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, OmegaConf\nSuccessfully installed OmegaConf-2.3.0 antlr4-python3-runtime-4.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":407},"id":"qRGdkO_hdx3X","outputId":"1e422a4f-4ce5-4716-e300-273992cca229"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bSV0tso9-cT","outputId":"d08ef5d0-d04d-48bd-b01c-01ac5928472c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Check PyTorch version and GPU availability\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"CUDNN version: {torch.backends.cudnn.version() if torch.cuda.is_available() else 'N/A'}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c3cSTQq-MxZ","outputId":"d671a39f-0acf-45c3-d79b-b3f218b1ba9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ninja --version","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8s9Mui-x-OSp","outputId":"4d18355b-f805-4e6b-b37a-9a8dce315b05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo $?","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SK3_noPf-fap","outputId":"ae3449d8-d4e7-4513-c524-72cec768fe50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install flash-attn --no-build-isolation","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJF4MDcL-kON","outputId":"efe477cc-17a2-4980-e2ef-6ee5bef15619","execution":{"iopub.status.busy":"2024-01-30T05:39:16.876388Z","iopub.execute_input":"2024-01-30T05:39:16.876691Z","iopub.status.idle":"2024-01-30T05:39:46.480165Z","shell.execute_reply.started":"2024-01-30T05:39:16.876664Z","shell.execute_reply":"2024-01-30T05:39:46.478981Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting flash-attn\n  Downloading flash_attn-2.5.0.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.0.0)\nCollecting einops (from flash-attn)\n  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn) (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->flash-attn) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.0-cp310-cp310-linux_x86_64.whl size=121788330 sha256=180f415c86683f9ff8ecbcee4333405107c550cc7d0d16830eb62f89438e46f3\n  Stored in directory: /root/.cache/pip/wheels/9e/c3/22/a576eb5627fb2c30dc4679a33d67d34d922d6dbeb24a9119b2\nSuccessfully built flash-attn\nInstalling collected packages: einops, flash-attn\nSuccessfully installed einops-0.7.0 flash-attn-2.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install /kaggle/input/flash-a/flash-attention-1.0.9/csrc/fused_dense_lib","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9T4UrSyv_tg8","outputId":"a7d6171f-186a-4a53-b364-d5eaa4e8edfe","execution":{"iopub.status.busy":"2024-01-30T05:39:46.481816Z","iopub.execute_input":"2024-01-30T05:39:46.482690Z","iopub.status.idle":"2024-01-30T05:39:59.479512Z","shell.execute_reply.started":"2024-01-30T05:39:46.482652Z","shell.execute_reply":"2024-01-30T05:39:59.478601Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/flash-a/flash-attention-1.0.9/csrc/fused_dense_lib\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: fused-dense-lib\n  Building wheel for fused-dense-lib (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[3 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m /opt/conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:398: UserWarning: There are no g++ version bounds defined for CUDA version 11.8\n  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n  \u001b[31m   \u001b[0m error: could not create '/kaggle/input/flash-a/flash-attention-1.0.9/csrc/fused_dense_lib/build': Read-only file system\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for fused-dense-lib\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for fused-dense-lib\nFailed to build fused-dense-lib\n\u001b[31mERROR: Could not build wheels for fused-dense-lib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install /content/flash_attention/flash-attention-main/csrc/xentropy\n# !pip install /content/flash_attention/flash-attention-main/csrc/rotary\n# !pip install /content/flash_attention/flash-attention-main/csrc/layer_norm","metadata":{"id":"9l4PMxfI_zcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom flash_attn.models.gpt import GPTLMHeadModel\n\nseqlen = 512\nhidden_dim = 2048\nnheads = 16\nn_layer = 24\nrotary_emb_fraction = 0.5\nconfig = GPT2Config(vocab_size=50257, n_positions=seqlen, n_embd=hidden_dim,\n                    n_layer=n_layer, n_head=nheads,\n                    scale_attn_by_inverse_layer_idx=True,\n                    rotary_emb_fraction=rotary_emb_fraction,\n                    use_flash_attn=True, fused_mlp=True,\n                    fused_bias_fc=True, fused_dropout_add_ln=True,\n                    pad_vocab_size_multiple=8)\nmodel = GPTLMHeadModel(config)","metadata":{"id":"FLVhRoZHLaf_","colab":{"base_uri":"https://localhost:8080/","height":367},"outputId":"e16f753f-5d72-4e6a-c038-fe64a365a425","execution":{"iopub.status.busy":"2024-01-25T13:34:32.567379Z","iopub.execute_input":"2024-01-25T13:34:32.567709Z","iopub.status.idle":"2024-01-25T13:34:37.574590Z","shell.execute_reply.started":"2024-01-25T13:34:32.567681Z","shell.execute_reply":"2024-01-25T13:34:37.573236Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m rotary_emb_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      9\u001b[0m config \u001b[38;5;241m=\u001b[39m GPT2Config(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m, n_positions\u001b[38;5;241m=\u001b[39mseqlen, n_embd\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[1;32m     10\u001b[0m                     n_layer\u001b[38;5;241m=\u001b[39mn_layer, n_head\u001b[38;5;241m=\u001b[39mnheads,\n\u001b[1;32m     11\u001b[0m                     scale_attn_by_inverse_layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     fused_bias_fc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fused_dropout_add_ln\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m                     pad_vocab_size_multiple\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTLMHeadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flash_attn/models/gpt.py:580\u001b[0m, in \u001b[0;36mGPTLMHeadModel.__init__\u001b[0;34m(self, config, process_group, device, dtype)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_group \u001b[38;5;241m=\u001b[39m process_group\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_word_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    582\u001b[0m lm_head_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flash_attn/models/gpt.py:464\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[0;34m(self, config, process_group, device, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m ParallelGPT2Embeddings(\n\u001b[1;32m    449\u001b[0m         config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    450\u001b[0m         vocab_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# We change the order of dropout, residual and layer norm:\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Instead of LN -> Attn / MLP -> Dropout -> Add, we do:\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# Dropout -> Add -> LN -> Attn / MLP, returning both the residual branch (output of Add) and\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# the main branch (output of MLP). The model definition is unchanged, but the mapping of the\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# nn.Dropout probabilities are changed.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# This is for performance reason: we can fuse dropout + add + layer_norm.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 464\u001b[0m     [\n\u001b[1;32m    465\u001b[0m         create_block(config, layer_idx\u001b[38;5;241m=\u001b[39mi, process_group\u001b[38;5;241m=\u001b[39mprocess_group, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    467\u001b[0m     ]\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    469\u001b[0m rotary_emb_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary_emb_fraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rotary_emb_fraction \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:  \u001b[38;5;66;03m# Tie all the RotaryEmbedding modules to share the same cos/sin cache\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flash_attn/models/gpt.py:465\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m ParallelGPT2Embeddings(\n\u001b[1;32m    449\u001b[0m         config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    450\u001b[0m         vocab_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    455\u001b[0m     )\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# We change the order of dropout, residual and layer norm:\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Instead of LN -> Attn / MLP -> Dropout -> Add, we do:\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# Dropout -> Add -> LN -> Attn / MLP, returning both the residual branch (output of Add) and\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# the main branch (output of MLP). The model definition is unchanged, but the mapping of the\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;66;03m# nn.Dropout probabilities are changed.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# This is for performance reason: we can fuse dropout + add + layer_norm.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    464\u001b[0m     [\n\u001b[0;32m--> 465\u001b[0m         \u001b[43mcreate_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    467\u001b[0m     ]\n\u001b[1;32m    468\u001b[0m )\n\u001b[1;32m    469\u001b[0m rotary_emb_fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary_emb_fraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rotary_emb_fraction \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:  \u001b[38;5;66;03m# Tie all the RotaryEmbedding modules to share the same cos/sin cache\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flash_attn/models/gpt.py:264\u001b[0m, in \u001b[0;36mcreate_block\u001b[0;34m(config, layer_idx, process_group, device, dtype)\u001b[0m\n\u001b[1;32m    262\u001b[0m sequence_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m mixer_cls \u001b[38;5;241m=\u001b[39m create_mixer_cls(config, layer_idx, process_group\u001b[38;5;241m=\u001b[39mprocess_group, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[0;32m--> 264\u001b[0m mlp_cls \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mlp_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m use_rms_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrms_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    266\u001b[0m norm_cls \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    267\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_rms_norm \u001b[38;5;28;01melse\u001b[39;00m RMSNorm,\n\u001b[1;32m    268\u001b[0m     eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlayer_norm_epsilon,\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    270\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/flash_attn/models/gpt.py:219\u001b[0m, in \u001b[0;36mcreate_mlp_cls\u001b[0;34m(config, layer_idx, process_group, device, dtype)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused_mlp:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m FusedMLP \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused_dense is not installed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m     activation \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu_approx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mactivation_function\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu_new\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu_fast\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu_approx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu_pytorch_tanh\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mactivation_function\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    226\u001b[0m     mlp_cls \u001b[38;5;241m=\u001b[39m FusedMLP \u001b[38;5;28;01mif\u001b[39;00m process_group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ParallelFusedMLP\n","\u001b[0;31mImportError\u001b[0m: fused_dense is not installed"],"ename":"ImportError","evalue":"fused_dense is not installed","output_type":"error"}]},{"cell_type":"code","source":"print(model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fGtkfTgLzRd","outputId":"83144c2c-02e8-43fb-ab83-1101386878c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.dataloader import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom pytorch_lightning import LightningDataModule\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nimport pytorch_lightning as pl\n\nimport wandb","metadata":{"id":"8A58_LRqWC3R","colab":{"base_uri":"https://localhost:8080/","height":397},"outputId":"7fc5db61-1fcb-4841-eb97-958c5fc5dd20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata =load_dataset(\"wikimedia/wikisource\", \"20231201.en\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZD3CnLdbb0b","outputId":"4ca63e0a-d6cb-4f47-a31b-07338c59a15b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWzaqGcCsnP3","outputId":"b2a11616-bb45-4e8c-eb8c-ddb716c8f897"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2Tokenizer\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.utils.data import DataLoader, Dataset, random_split\ntokenizer =  GPT2Tokenizer.from_pretrained(\"gpt2\")\nif tokenizer.pad_token is None:\n  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\nclass Wikidata(Dataset):\n    def __init__(self, dataset, max_length=512,tokenizer=tokenizer,data_collator=data_collator):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.data_collator = data_collator\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        # Get text from the dataset\n        text = self.dataset[idx][\"text\"]  # Access the list using idx and then get \"text\" from the dictionary\n\n        # Tokenize the text using the GPT-2 tokenizer\n        inputs = self.tokenizer(\n            text,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\"\n        )\n\n        # For GPT-2 language modeling, the target is the same as the input shifted by one position\n        labels = inputs[\"input_ids\"].clone()\n        labels[:, :-1] = inputs[\"input_ids\"][:, 1:]\n        labels[:, -1] = self.tokenizer.eos_token_id  # Set the last token to the EOS token\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"labels\": labels.squeeze()\n        }\n","metadata":{"id":"KaAFRDtTXUJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WikiDataModule(pl.LightningDataModule):\n    def __init__(self, dataset, batch_size=1, max_length=512, val_split=0.1, test_split=0.1, num_workers=2):\n        super().__init__()\n        self.dataset = dataset['train']\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.val_split = val_split\n        self.test_split = test_split\n        self.num_workers = num_workers\n\n    def setup(self, stage=None):\n        train_size = int(len(self.dataset) * (1 - self.val_split - self.test_split))\n        val_size = int(len(self.dataset) * self.val_split)\n        test_size = len(self.dataset) - train_size - val_size\n        self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.dataset, [train_size, val_size, test_size])\n        self.train_dataset = Wikidata(self.train_dataset, self.max_length)\n        self.val_dataset = Wikidata(self.val_dataset, self.max_length)\n        self.test_dataset = Wikidata(self.test_dataset, self.max_length)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, collate_fn=self.train_dataset.data_collator)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, collate_fn=self.val_dataset.data_collator)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, collate_fn=self.test_dataset.data_collator)\n","metadata":{"id":"ukjzcNhHcGi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1\nmax_length = 512\nnum_workers = 4\n\nwikidatamodule = WikiDataModule(data, batch_size=batch_size, max_length=max_length, num_workers=num_workers)\nwikidatamodule.setup()\ntrain_dataloader = wikidatamodule.train_dataloader()\nval_dataloader = wikidatamodule.val_dataloader()","metadata":{"id":"Ue51O83pcH7l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom flash_attn.models.gpt import GPTLMHeadModel\nimport pytorch_lightning as pl\n\nseqlen = 2048\nhidden_dim = 2048\nnheads = 16\nn_layer = 24\nrotary_emb_fraction = 0.5\nconfig = GPT2Config(vocab_size=50257, n_positions=seqlen, n_embd=hidden_dim,\n                    n_layer=n_layer, n_head=nheads,\n                    scale_attn_by_inverse_layer_idx=True,\n                    rotary_emb_fraction=rotary_emb_fraction,\n                    use_flash_attn=True, fused_mlp=True,\n                    fused_bias_fc=True, fused_dropout_add_ln=True,\n                    pad_vocab_size_multiple=8)\nmodel = GPTLMHeadModel(config)\n\n\n","metadata":{"id":"3xcXESfRc9EW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport pytorch_lightning as pl\nfrom transformers.models.gpt2.configuration_gpt2 import GPT2Config\nfrom flash_attn.models.gpt import GPTLMHeadModel\n\nclass LightningGPTModel(pl.LightningModule):\n    def __init__(self, config):\n        super(LightningGPTModel, self).__init__()\n        self.model = GPTLMHeadModel(config)\n\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n        return outputs.loss\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels = batch[\"labels\"]\n\n        outputs = self(input_ids, attention_mask, labels=labels)\n        loss = outputs\n\n        return loss\n\n\n    def configure_optimizers(self):\n        # Define your optimizer and scheduler (if any) here\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-5)\n        return optimizer\n\n# Your existing code to set up the model configuration\nseqlen = 512\nhidden_dim = 2048\nnheads = 16\nn_layer = 24\nrotary_emb_fraction = 0.5\nconfig = GPT2Config(vocab_size=50257, n_positions=seqlen, n_embd=hidden_dim,\n                    n_layer=n_layer, n_head=nheads,\n                    scale_attn_by_inverse_layer_idx=True,\n                    rotary_emb_fraction=rotary_emb_fraction,\n                    use_flash_attn=True, fused_mlp=True,\n                    fused_bias_fc=True, fused_dropout_add_ln=True,\n                    pad_vocab_size_multiple=8)\n\n# Create an instance of the LightningGPTModel\nlightning_model = LightningGPTModel(config)","metadata":{"id":"xVGTzKMVw-yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass LightningGPTModel(pl.LightningModule):\n    def __init__(self, config):\n        super(LightningGPTModel, self).__init__()\n        self.model = GPTLMHeadModel(config)\n\n    def forward(self, input_ids, attention_mask=None):\n        return self.model(input_ids)\n\n    def training_step(self, batch, batch_idx):\n        # The 'labels' tensor is automatically passed to this method by PyTorch Lightning\n        inputs = {\"input_ids\": batch[\"input_ids\"]}\n\n        # Handle attention_mask separately if your model requires it\n        if \"attention_mask\" in batch:\n            inputs[\"attention_mask\"] = batch[\"attention_mask\"]\n\n        outputs = self.model(**inputs, labels=batch[\"labels\"])\n        loss = outputs.loss\n        return loss\n\n\n\n\n\n\n\n","metadata":{"id":"do_Ut_myfwjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lightning_model)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmPg_k6Xu8j1","outputId":"0213b017-788c-4c11-ac00-4519b3b77384"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_module = WikiDataModule(dataset=data, batch_size=1, max_length=512, val_split=0.1, test_split=0.1, num_workers=2)\ndata_module.setup()  # Call setup to initialize train, val, and test datasets\n\ntrainer_config = {\n    'max_epochs': 5,\n\n    # 'gpus': 1,\n    # 'accumulate_grad_batches': 1,\n    # # Add other trainer configurations as needed\n}\n\n# Step 4: Instantiate Trainer\ntrainer = pl.Trainer(**trainer_config,detect_anomaly=True)\n\n# Step 5: Train the Model\ntrainer.fit(lightning_model, datamodule=data_module)\n\n# Step 6: (Optional) Test the Model\ntrainer.test(datamodule=data_module)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"id":"zXReyCm4cS-_","outputId":"23a347d6-e6f7-48ff-e0b1-4090c1463876"},"execution_count":null,"outputs":[]}]}